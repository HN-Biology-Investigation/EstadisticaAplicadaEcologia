---
title: ''
output: 
       html_document:
         css: custom.css
header-includes:
  - \usepackage{float}
  - \floatstyle{boxed}
  - \restylefloat{figure}
  
---

<div style="display:flex; align-items:center;">
  <h1 style="margin:0;">Clase 1: Comandos Básicos y Exploración de Datos</h1>
  <img src="HN Cursos publicidad/HN Biology Inv Circle.jpg" alt="" style="width:100px; margin-left:20px;">
</div>

Esta clase está diseñada para enseñarte los comandos básicos en R, incluyendo la creación de variables, tablas y la importación de bases de datos. Además, aprenderás a explorar y analizar tus datos de manera apropiada, siguiendo la metodología de Zuur **et al**. (2010). Con el tiempo, podrás utilizar estas habilidades básicas en R para realizar análisis más complejos y obtener resultados acordes a tu pregunta de investigación.


### Contenido {.tabset .tabset-pills}

#### Operaciones Basicas

- Adición (suma)

```{r}
2 + 2
```


- Sustracción (resta)

```{r}
2 - 2
```

- Multiplicación 

```{r}
2 * 2
```

- División

```{r}
2 / 2
```

- Raíz cuadrada  

::: {.notes}

Observe que para la raíz cuadrada se utiliza el comando `sqrt` y el valor del que se quiere obtener la raíz cuadrada se coloca dentro de los paréntesis `( )`.

```{r}
sqrt(2)
```

:::

Ahora que conocemos algunas de las operaciones básicas, podemos resolver algunos ejercicios.

:::{.notes}

**Ejercicio 1**

Utilizando la ecuación de regresión linear, para la cual la `pendiente` tiene un valor de 0.3 y el `intercepto` un valor de 34.5, resolver cuanto sería el valor de `y` si `x` vale 36.4.

  $y = intercepto + slope(x)$

Solución:

```{r}
34.5 + 0.3*36.4
```


:::

#### Creación de variables

Comencemos creando variables numéricas con un único valor

```{r}
x <- 5 # En este caso, x tendrá el valor de 5
```


```{r}
y <- -3 # En este caso, y tendrá el valor de -3
```

:::{.notes}

Si queremos crear variables numéricas con varios valores, tendremos que crear un vector utilizando el comando `c()`, en donde cada valor estará separa por coma.

```{r}
Peso <- c(12.5, 23, 12.6, 18, 7, 18.3, 23, 40, 21,8)
```

Observe que en este caso las comas se parados los valores, y los puntos solo se utilizan para las cifras decimales. La variable `Peso` tendrá 10 observaciones.

Otra forma de crear un vector es utilizando el signo `:` entre los valores del inicio y final.


```{r}

Altura <- c(1:1000)
```

En este caso la variable `Altura` tendrá 1000 observaciones

:::

Ahora, ¿cómo podemos crear una variable con números aleatorios?, como podemos crear una variable con distribución normal?

:::{.notes}

**Variable con numeros alatorios**

Primero podríamos utilizar el comando `sample()` para crear números aleatorios sin una distribución en especifica, dando el número de inicio y final, y la cantidad de observaciones que deseamos.

```{r}
Aleatorios <- sample(1:1000, size = 50)
```

En este ejemplo observe que, 1 y 1000 es el rango entre el cual estarán 50 observaciones.

**Variable con distribucion normal**

Podemos crear variables con una distribución normal utilizando el comando `rnorm()`, en este caso tendremos que especificar la media y la desviación estándar.

```{r}
Dnormal <- rnorm(50, mean = 30, sd = 2.5)
```

Para este ejemplo, hemos creado 50 observaciones, con una media de 30 y una desviación estándar de 2.5

:::


El siguiente paso es la creación de variables categorías, variables que incluyen `categorias`. Para crear una variable de categorías tendremos que colocar cada observación entre comillas `" "`

```{r}
Variabley <- "y"
```

En este ejemplo la `variabley` solo tiene la observación `y`. Si quermos más observaciones podemos utilizar el comando `c()`.

```{r}
Seccion <- c("1A", "1B", "3A", "2B", "2A", "3A")
```

#### Cracion de tablas

Ya que aprendimos como crear variables numéricas y categóricas, ahora podemos crear tablas. Para crear una tabla en R debemos de asegurar que nuestras variables presenten la misma cantidad de observaciones.

:::{.notes}

**Creación de tabla con 10 observaciones**

Primero crearemos nuestras variables

```{r}
Temperatura <- sample(20:35, 10)

Riqueza <- rnorm(10, mean = 5, sd= 3)

Franja <- c("A", "A", "C", "C", "A", "B", "B", "B", "A", "C")
```

Ahora utilizando el comando `data.frame()` podemos crear nuestra tabla

```{r}
Tabla1 <- data.frame(Franja, Temperatura, Riqueza)
```


Observemos nuestra primera tabla

```{r, echo=FALSE}

knitr::kable(Tabla1)
```

:::



#### Cargar base de datos

Para cargar una base de datos de nuestro pc, necesitamos sabe exactamente la ubicación de la base de datos. Por ejemplo para cargar la base de datos `bird.sta` que se encuentra en la carpeta `data`, de nuestro proyecto de R. Podemos usar el comando `read.csv()` ya que esta base de datos está delimitada por coma.


:::{.notes}

**Ejemplo para cargar una base de datos**

```{r, eval=FALSE}

Aves.sta <- read.csv("data/bird.sta.csv")
```

Observe que el nombre de la base de datos es `bird.sta`, el formato es `.csv` y se encuentra en la carpeta `data`, esta base de datos fue guardada en `Aves.sta` en nuestro ambiente en R

:::

#### Estadistica descriptiva

Para los siguientes ejemplos utilizaremos la base de datos `penguins` del paquete `palmerpenguins`


:::{.notes} 

**Cargar la base de datos penguins**

Primero debemos cargar el paquete `palmerpenguins`

```{r}
library(palmerpenguins)
```

Una vez cargada el paquete podemos cargar a la base de datos `penguins`

```{r}
data("penguins")
```

:::


:::{.notes}

Calcular la media de cada columna del data frame

```{r}
sapply(penguins[, 3:6], mean, na.rm = TRUE)
```

Observe que que estamos seleccionando la columna 3 a la 6 (ya que contienen variables numericas), y el nar.rm = TRUE, es para que reconosca que existen NA en nuestra base de datos.

:::

:::{.notes}

Calcular la media de la masa de los pingüinos para cada especie

```{r}
tapply(penguins$body_mass_g, penguins$species, mean, na.rm =TRUE )
```
Observe que que estamos seleccionando la columna `body_mass_g` para estimar la media condicionada por la variable `species`, el nar.rm = TRUE, es para que reconosca que existen NA en nuestra base de datos.

:::

#### Exploración de datos (Zuur et al. 2010)

De acuerdo con [Zuur et al. (2010)](papers/Zuur2010.pdf), hay seis pasos que se deben seguir en la exploración de datos con el fin de facilitar la elección del método estadístico a utilizar.

Para este ejercicio utilizaremos la base de datos del estudio de [O'Neill et al. (2018)](papers/Nest_size.pdf).

para cargar la base de datos utilizares el comando:

```{r, eval=FALSE}
cyanistes <- read.table("data/cyanistes.txt", header = TRUE)
```

```{r, echo=FALSE}
cyanistes <- read.table(here::here("docs/data","cyanistes.txt"), header = TRUE)
```

Comencemos inspeccionando la estructura de la base de datos con la función 'str()'

```{r}
str(cyanistes)
```
¿Existen valores nulos (NAs) en el conjunto de datos?

```{r}
colSums(is.na(cyanistes))
```


##### 1) Valores extremos (outliers) en la variable respuesta y predictora.

Análisis visual:

```{r}
boxplot(depth ~ zone,
        ylab= "Profundidad del nido",
        xlab = "Zona del bosque",
        data= cyanistes,
        outpch = 16,
        las= 1)
```

Análisis estadístico:

Para este ejercicio, utilizaremos el paquete `outliers`. Los **::** después de outliers indican que usaremos la función `grubbs.test` del paquete `outliers` sin necesidad de cargarlo previamente.

```{r}
outliers::grubbs.test(cyanistes$depth, type = 10)

```

```{r}
outliers::grubbs.test(cyanistes$height, type = 11)

```


```{r}
outliers::grubbs.test(cyanistes$day, type = 10)

```


##### 2) Normalidad y homogeneidad en la variable respuesta.

Un supuesto de algunas pruebas estadísticas es que la variable respuesta sigue una distribución normal. Para verificar este supuesto, se puede utilizar una prueba de normalidad, como la prueba de Shapiro-Wilk. La hipótesis nula de esta prueba establece que los datos se distribuyen normalmente. Si el valor p es menor que 0.05 (p < 0.05), se rechaza la hipótesis nula.

```{r}
shapiro.test(cyanistes$depth)
```

La homogeneidad de varianzas es otro supuesto importante en muchas pruebas estadísticas. Para visualizar la homogeneidad de varianzas en una variable categórica, se puede utilizar un gráfico de caja. Sin embargo, un gráfico de dispersión es más adecuado para visualizar la homogeneidad en variables numéricas.

```{r}
library(ggplot2)
ggplot(data= cyanistes, aes(x= height, y= depth)) +
  geom_point()+
  geom_hline(yintercept = 0.33, linetype = "dashed")+
  geom_vline(xintercept = 2.19, linetype = "dashed")
```

Existen varias pruebas estadísticas para evaluar la homogeneidad de varianzas, como la prueba de Bartlett, la prueba F y la prueba de Levene.

```{r}
car::leveneTest(depth ~ zone, data = cyanistes)
```


##### 3) Exceso de ceros en la variable respuesta.

Los ceros no pueden ser omitidos de un conjunto de datos; sin embargo, un exceso de ceros en la variable respuesta, conocido como inflación de ceros, puede causar problemas en los análisis. Una forma de evaluar la inflación de ceros es calculando el porcentaje de ceros en la variable:

```{r}
sum(cyanistes$depth == 0, na.rm = TRUE) * 100 / nrow(cyanistes)
```
En este caso, no tenemos problemas con los ceros. Sin embargo, de presentarse, tendríamos que buscar alternativas para manejarlos, lo cual abordaremos más adelante.

##### 4) Multicolinealidad entre variables predictoras.

La multicolinealidad entre variables predictoras se refiere a una situación en la que dos o más variables independientes en un modelo de regresión están altamente correlacionadas entre sí. Esto puede dificultar la estimación de los coeficientes del modelo, ya que hace que sea complicado determinar el efecto individual de cada variable en la variable dependiente. La multicolinealidad puede inflar los errores estándar de los coeficientes, lo que lleva a inferencias estadísticas poco confiables, como valores p engañosos y una disminución en la capacidad predictiva del modelo. 

```{r, message=FALSE, warning=FALSE}
GGally::ggpairs(cyanistes[,2:7])
```



##### 5) Relaciones entre variables respuesta y predictoras.

Las relaciones entre variables respuesta y predictoras se refieren a cómo la variable dependiente (respuesta) se ve afectada por las variables independientes (predictoras) en un conjunto de datos. Examinar estas relaciones es fundamental en la exploración de datos porque permite identificar patrones, tendencias y correlaciones que pueden influir en el comportamiento de la variable respuesta. Comprender estas relaciones ayuda a construir modelos predictivos más precisos, a formular hipótesis y a guiar la interpretación de resultados, facilitando así la toma de decisiones informadas en el análisis estadístico y en la investigación.

```{r}
ggplot(data = cyanistes, aes(x = height, y = depth)) +
  geom_point() +
  geom_smooth(method = lm) +
  facet_wrap(~zone) +
  theme_classic()
```


##### 6) Independencia en las observaciones de la variable respuesta.

La independencia en las observaciones de la variable respuesta se refiere a la suposición de que cada observación en el conjunto de datos es independiente de las demás. Esto significa que el valor de una observación no debe influir en el valor de otra. La independencia es crucial en el análisis estadístico porque garantiza que las inferencias y los resultados sean válidos. Si las observaciones no son independientes, puede haber una correlación no reconocida entre ellas, lo que puede conducir a errores en la estimación de parámetros, aumento de la varianza de los errores y, en última instancia, a conclusiones incorrectas sobre las relaciones entre variables. Por lo tanto, verificar la independencia de las observaciones es un paso esencial en la exploración de datos y en la construcción de modelos estadísticos.



